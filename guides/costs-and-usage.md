# Costs and Usage

This guide explains how to track token consumption and context usage when using the Kimi Agent SDK.

## Usage Structure

The SDK provides usage information through the `Usage` struct:

```go
type Usage struct {
    Context float64         // Context window usage (0.0 to 1.0)
    Tokens  wire.TokenUsage // Detailed token counts
}
```

### TokenUsage Fields

```go
type TokenUsage struct {
    InputOther         int `json:"input_other"`          // Input tokens (non-cached)
    Output             int `json:"output"`               // Output tokens generated
    InputCacheRead     int `json:"input_cache_read"`     // Tokens read from cache
    InputCacheCreation int `json:"input_cache_creation"` // Tokens used to create cache
}
```

## Getting Usage After Turn Completion

The simplest way to get usage is after consuming all messages:

```go
turn, _ := session.Prompt(ctx, wire.NewStringContent("Hello!"))

// Consume all messages
for step := range turn.Steps {
    for msg := range step.Messages {
        // Process messages...
    }
}

// Get usage
usage := turn.Usage()
if usage != nil {
    fmt.Printf("Context Usage: %.2f%%\n", usage.Context * 100)
    fmt.Printf("Input Tokens: %d\n", usage.Tokens.InputOther)
    fmt.Printf("Output Tokens: %d\n", usage.Tokens.Output)
    fmt.Printf("Cache Read: %d\n", usage.Tokens.InputCacheRead)
    fmt.Printf("Cache Creation: %d\n", usage.Tokens.InputCacheCreation)
}
```

## Understanding the Metrics

### Context Usage

Context usage indicates how much of the model's context window is being used:

- `0.0` = Empty context
- `0.5` = 50% utilized
- `1.0` = Context window is full

When context approaches 100%, older messages may be compacted or removed.

### Token Types

| Token Type | Description |
|------------|-------------|
| **InputOther** | Regular input tokens (your prompts, system messages) |
| **Output** | Tokens generated by the model |
| **InputCacheRead** | Tokens retrieved from prompt cache (faster, cheaper) |
| **InputCacheCreation** | Tokens used to populate the cache |

### Cost Calculation

Typical pricing is based on:
- Input tokens: `InputOther + InputCacheCreation`
- Cached input tokens: `InputCacheRead` (usually discounted)
- Output tokens: `Output`

Check your provider's pricing for exact rates.

## Tracking Across Multiple Turns

For multi-turn conversations, accumulate usage:

```go
type SessionUsage struct {
    TotalInputOther         int
    TotalOutput             int
    TotalCacheRead          int
    TotalCacheCreation      int
    Turns                   int
}

func (s *SessionUsage) Add(usage *kimi.Usage) {
    if usage == nil {
        return
    }
    s.TotalInputOther += usage.Tokens.InputOther
    s.TotalOutput += usage.Tokens.Output
    s.TotalCacheRead += usage.Tokens.InputCacheRead
    s.TotalCacheCreation += usage.Tokens.InputCacheCreation
    s.Turns++
}

// Usage
var totalUsage SessionUsage

turn1, _ := session.Prompt(ctx, wire.NewStringContent("First question"))
// ... consume messages ...
totalUsage.Add(turn1.Usage())

turn2, _ := session.Prompt(ctx, wire.NewStringContent("Second question"))
// ... consume messages ...
totalUsage.Add(turn2.Usage())

fmt.Printf("Total turns: %d\n", totalUsage.Turns)
fmt.Printf("Total input: %d\n", totalUsage.TotalInputOther)
fmt.Printf("Total output: %d\n", totalUsage.TotalOutput)
```

## Complete Example

```go
package main

import (
    "context"
    "fmt"
    "os"

    kimi "github.com/MoonshotAI/kimi-agent-sdk/go"
    "github.com/MoonshotAI/kimi-agent-sdk/go/wire"
)

func main() {
    session, err := kimi.NewSession(
        kimi.WithAPIKey(os.Getenv("KIMI_API_KEY")),
    )
    if err != nil {
        panic(err)
    }
    defer session.Close()

    turn, err := session.Prompt(context.Background(),
        wire.NewStringContent("Explain quantum computing in detail"))
    if err != nil {
        panic(err)
    }

    // Consume all messages
    for step := range turn.Steps {
        for msg := range step.Messages {
            if cp, ok := msg.(wire.ContentPart); ok && cp.Type == wire.ContentPartTypeText {
                fmt.Print(cp.Text)
            }
        }
    }
    fmt.Println()

    if err := turn.Err(); err != nil {
        fmt.Printf("Error: %v\n", err)
        return
    }

    // Get usage after turn completion
    fmt.Println("\n=== Usage Summary ===")
    usage := turn.Usage()
    if usage != nil {
        fmt.Printf("Context Usage: %.2f%%\n", usage.Context * 100)
        fmt.Printf("Input Tokens: %d\n", usage.Tokens.InputOther)
        fmt.Printf("Output Tokens: %d\n", usage.Tokens.Output)
        fmt.Printf("Cache Read: %d\n", usage.Tokens.InputCacheRead)
        fmt.Printf("Cache Creation: %d\n", usage.Tokens.InputCacheCreation)

        totalInput := usage.Tokens.InputOther + usage.Tokens.InputCacheCreation
        fmt.Printf("\nTotal Billable Input: %d\n", totalInput)
        fmt.Printf("Total Output: %d\n", usage.Tokens.Output)
    }
}
```

## Tips for Cost Optimization

1. **Monitor context usage** - High context usage means more tokens are being processed
2. **Leverage caching** - Repeated prompts with the same prefix benefit from cache
3. **Use concise prompts** - Shorter prompts = fewer input tokens
4. **Check cache hit rate** - High `InputCacheRead` relative to `InputOther` is good
5. **Set limits** - Use `LoopControl.MaxStepsPerRun` to prevent runaway costs

## Compaction Events

When context gets full, the CLI may compact history. These events are pushed to `step.Messages`:

```go
for step := range turn.Steps {
    for msg := range step.Messages {
        switch msg.(type) {
        case wire.CompactionBegin:
            fmt.Println("Starting context compaction...")
        case wire.CompactionEnd:
            fmt.Println("Compaction completed")
        }
    }
}
```

After compaction, older messages are summarized to free up context space.
